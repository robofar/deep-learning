{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d65c7b4-8cf1-4d22-a849-f579862855a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dca5bb-81b0-406e-b93f-bf50cfbdfa8d",
   "metadata": {},
   "source": [
    "# Auto Gradient\n",
    "\n",
    "By enabling ``grad`` for ``torch.Tensor``, we are enabling option that ``torch`` will calculate gradient for all leaf nodes, and store them in ``.grad`` attribute of Tensor.\n",
    "\n",
    "Non-Leaf node needs to be scalar if we want to compute gradient of leaf nodes.\n",
    "\n",
    "This is usually the case, since leaf node is some loss function, which is nothing but a scalar.\n",
    "\n",
    "When we say, gradients will be calculated for non-leaf nodes, we mean gradinets of the leaf node with respect to all elements of all non-leaf nodes (e.g. leaf node is some loss function denoted as $L$).\n",
    "\n",
    "This is used during training, where we need gradients of Loss function $L$ with respect to all parameters to optimize our parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52ba4d8-fbaf-4904-bebd-ada6ed64e9c9",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "``c = torch.sum(a + b) ``\n",
    "\n",
    "If we enable gradient calculations for ``a`` and ``b``, following gradients will be calculated:\n",
    " - ``dc/da`` - for each element of a indenpendently\n",
    " - ``dc/db`` - for each element of b indenpendently\n",
    "\n",
    "``c`` needs to be scalar (e.g. for training network that is our Loss Function - which is scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db1b7f75-117f-4ecb-a900-6cf77ea0bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "# a.requires_grad_()\n",
    "# a.requires_grad = True\n",
    "b = torch.randn(2, 2, requires_grad=True)\n",
    "c = torch.sum(a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b61cbe-3fbe-44ab-86f0-0e764ee3d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfbe452a-8920-47f5-9c06-5b66d4dd5168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6104b71-c44b-4fa1-8d52-08e6c7e1dc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ca7fd40-74c5-42ec-8f45-cd8748cc83ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-67af56c47ba0>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642991888/work/build/aten/src/ATen/core/TensorBody.h:480.)\n",
      "  c.grad\n"
     ]
    }
   ],
   "source": [
    "c.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05f67424-07df-42fc-aece-3b1a1c985c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0796,  0.3734],\n",
       "        [ 0.4797, -1.6013]], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "180af793-affd-425b-9a55-a6b408880793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8128, -1.8824],\n",
       "        [ 1.6154, -0.2534]], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91cd8bd2-ab49-4c24-a294-4a90584a573c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.0019, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47caf2e7-cc43-4157-a691-6a65da6b44b4",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "``b = a ** 2``\n",
    "\n",
    "``c = b.mean()``\n",
    "\n",
    "``grad`` will be computed only for ``a`` not for ``b`` (I was expecting to be computed for ``b`` as well (clarify this)/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e91d9cfb-87d8-4d02-898e-6290c7e0beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "a = torch.ones((2,2)).requires_grad_()\n",
    "b = a ** 2\n",
    "c = b.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7610de65-f6c2-4b1d-82c7-39ff87cbfc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5753b6d1-0c7c-4709-ad4f-464120e84861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000],\n",
       "        [0.5000, 0.5000]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b17186c-f02b-4532-bd31-797385f59acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-da255cd7360a>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642991888/work/build/aten/src/ATen/core/TensorBody.h:480.)\n",
      "  b.grad\n"
     ]
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f68b468-e225-4f80-ba0e-5ff4061d2903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-67af56c47ba0>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642991888/work/build/aten/src/ATen/core/TensorBody.h:480.)\n",
      "  c.grad\n"
     ]
    }
   ],
   "source": [
    "c.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1b1f8eb-81d1-472b-bd2e-2f8743eacf2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5abd8791-37d0-450a-8363-308c23014d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "959df4d5-cf57-4470-b510-25a004a11190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf61330-9045-4a3b-8b90-981f83e6625e",
   "metadata": {},
   "source": [
    "#### Summary Note: \n",
    "When you finish your ``forward pass`` you call ```.backward()``` and compute backpropagation and all the gradients will becomputed automatically. The gradient for tensor will be accumulated into ```.grad``` attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e91196-241b-448f-a5b0-9e0ed1f2b12d",
   "metadata": {},
   "source": [
    "### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bce488b1-255b-4808-8e1f-738d9ddf13af",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.Tensor([1]).requires_grad_()\n",
    "x2 = torch.Tensor([2]).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03025ab4-74ab-4649-a9a7-bf3c942bba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = (5*x1) + (3*x2)\n",
    "y2 = (2*x1) + (1*x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f062efab-4291-4609-a348-13101f117890",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y1*y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cefb29f4-dcef-4403-9101-22ddb28e0661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], requires_grad=True)\n",
      "tensor([2.], requires_grad=True)\n",
      "tensor([11.], grad_fn=<AddBackward0>)\n",
      "tensor([4.], grad_fn=<AddBackward0>)\n",
      "tensor([44.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(x1)\n",
    "print(x2)\n",
    "print(y1)\n",
    "print(y2)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e97124ec-ead8-45d0-be97-7865a1a6dd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c5dc82d-fe3b-4782-b6ac-05fe2601934a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([42.])\n",
      "tensor([23.])\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-75be95a57e25>:3: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642991888/work/build/aten/src/ATen/core/TensorBody.h:480.)\n",
      "  print(y1.grad)\n",
      "<ipython-input-23-75be95a57e25>:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642991888/work/build/aten/src/ATen/core/TensorBody.h:480.)\n",
      "  print(y2.grad)\n",
      "<ipython-input-23-75be95a57e25>:5: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642991888/work/build/aten/src/ATen/core/TensorBody.h:480.)\n",
      "  print(z.grad)\n"
     ]
    }
   ],
   "source": [
    "print(x1.grad)\n",
    "print(x2.grad)\n",
    "print(y1.grad)\n",
    "print(y2.grad)\n",
    "print(z.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d593b4a8-8eea-4856-825f-8146935beac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([42.])\n",
      "None\n",
      "------------------\n",
      "tensor([23.])\n",
      "None\n",
      "------------------\n",
      "None\n",
      "<AddBackward0 object at 0x7f033cf78ee0>\n",
      "------------------\n",
      "None\n",
      "<AddBackward0 object at 0x7f033cf78a30>\n",
      "------------------\n",
      "None\n",
      "<MulBackward0 object at 0x7f033cf78d90>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-b9d8db878a75>:7: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642991888/work/build/aten/src/ATen/core/TensorBody.h:480.)\n",
      "  print(y1.grad)\n",
      "<ipython-input-24-b9d8db878a75>:10: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642991888/work/build/aten/src/ATen/core/TensorBody.h:480.)\n",
      "  print(y2.grad)\n",
      "<ipython-input-24-b9d8db878a75>:13: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642991888/work/build/aten/src/ATen/core/TensorBody.h:480.)\n",
      "  print(z.grad)\n"
     ]
    }
   ],
   "source": [
    "print(x1.grad)\n",
    "print(x1.grad_fn)\n",
    "print('------------------')\n",
    "print(x2.grad)\n",
    "print(x2.grad_fn)\n",
    "print('------------------')\n",
    "print(y1.grad)\n",
    "print(y1.grad_fn)\n",
    "print('------------------')\n",
    "print(y2.grad)\n",
    "print(y2.grad_fn)\n",
    "print('------------------')\n",
    "print(z.grad)\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213b5900-2097-4f5d-a879-bdb32b0eeac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2859886-1a8d-4b06-877d-665a75162fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88f96ad5-a5e0-448a-bd9e-c34f92f4b181",
   "metadata": {},
   "source": [
    "#### **Important:** Future calls to backward will accumulate gradients into this vector\n",
    "\n",
    "#### - $y = \\frac{1}{4} \\sum_{i=1}^{4} 2 \\cdot x_i $\n",
    "#### - $\\frac{\\partial y}{\\partial x_i} = \\frac{1}{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afbcf612-b7e1-4510-bcb3-86677d521ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients at iteration 1:\n",
      "tensor([[0.5000, 0.5000],\n",
      "        [0.5000, 0.5000]])\n",
      "\n",
      "Gradients at iteration 2:\n",
      "tensor([[0.5000, 0.5000],\n",
      "        [0.5000, 0.5000]])\n",
      "\n",
      "Gradients at iteration 3:\n",
      "tensor([[0.5000, 0.5000],\n",
      "        [0.5000, 0.5000]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((2, 2)).requires_grad_()\n",
    "for i in range(3):\n",
    "    y = (2 * x).mean()\n",
    "    y.backward()\n",
    "    print(f\"Gradients at iteration {i+1}:\")\n",
    "    print(f\"{x.grad}\\n\")\n",
    "    x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d3e276-f18c-4526-9941-4ad98baaf972",
   "metadata": {},
   "source": [
    "### Gradient Calculation\n",
    " - $ y = \\frac{1}{N} \\sum_{i=1}^{N} u_i + 2 \\cdot v_i$ <br><br>\n",
    " - $ \\frac{\\partial y}{\\partial u_i} = \\frac{1}{N}$ <br> <br>\n",
    " - $ \\frac{\\partial y}{\\partial v_i} = \\frac{2}{N}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6655a50-fbba-4806-ab42-3f9bd565e7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.5000, grad_fn=<MeanBackward0>)\n",
      "True\n",
      "<MeanBackward0 object at 0x7f033cf78850>\n",
      "\n",
      "True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "tensor_one = torch.tensor([[1., 2.], [3., 4.]], requires_grad=True)\n",
    "tensor_two = torch.tensor([[5., 6.], [7., 8.]], requires_grad=True)  \n",
    "\n",
    "# By default, new tensor which is operation of some other tensor whose requires_grad is True, will also have requires_grad=True)\n",
    "final_tensor = (tensor_one + 2 * tensor_two).mean()\n",
    "print(final_tensor)\n",
    "print(final_tensor.requires_grad)\n",
    "print(final_tensor.grad_fn)\n",
    "print()\n",
    "print(tensor_one.requires_grad)\n",
    "print(tensor_one.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b017507d-18d8-4cea-9f91-0742c2b6b939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_one.grad = tensor([[0.2500, 0.2500],\n",
      "        [0.2500, 0.2500]])\n",
      "tensor_two.grad = tensor([[0.5000, 0.5000],\n",
      "        [0.5000, 0.5000]])\n",
      "final_tensor.grad = None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-a6f6f6da6b79>:9: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1666642991888/work/build/aten/src/ATen/core/TensorBody.h:480.)\n",
      "  print(f\"{final_tensor.grad = }\")\n"
     ]
    }
   ],
   "source": [
    "final_tensor.backward()\n",
    "\n",
    "# tensor_one = [[x1,x2],[x3,x4]]\n",
    "# tensor_one.grad = [[d_ft/d_x1, d_ft/d_x2], [d_ft/d_x3, d_ft/d_x4]]\n",
    "print(f\"{tensor_one.grad = }\")\n",
    "\n",
    "print(f\"{tensor_two.grad = }\")\n",
    "\n",
    "print(f\"{final_tensor.grad = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd08d8b4-8f13-403d-a982-cc1ea4a1b617",
   "metadata": {},
   "source": [
    "### Enabling and Disabling Gradient Calculations \n",
    "You can also stops autograd from tracking history on newly created tensors with *requires_grad=True* by: \n",
    " - wrapping the code block in **with torch.no_grad()**\n",
    " - x.reguires_grad_(False)\n",
    " - x.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2d5f0b9-5e9a-4048-a704-b2087501fcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = torch.Tensor([[1, 2, 3], \n",
    "                        [4, 5, 6]])\n",
    "tensor1.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1663a5c8-4856-4db0-b0de-6556c28b3e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_tensor =  tensor([[ 3.,  6.,  9.],\n",
      "        [12., 15., 18.]])\n",
      "requires_grad for tensor1 =  True\n",
      "requires_grad for new_tensor =  False\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    new_tensor = tensor1 * 3\n",
    "    print('new_tensor = ', new_tensor)\n",
    "    print('requires_grad for tensor1 = ', tensor1.requires_grad)\n",
    "    print('requires_grad for new_tensor = ', new_tensor.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00dfea84-930b-4b44-adf3-a6d8a0ff6406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1.requires_grad_(False)\n",
    "tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b80e513-f8e2-4485-943b-a4fc1c8548f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]], requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1.requires_grad_(True)\n",
    "tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39993c2d-5a4d-4797-a7ca-9f2f35ed81fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]], requires_grad=True)\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "new_tensor = tensor1.detach()\n",
    "print(tensor1)\n",
    "print(new_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4afd44-a047-4aa2-9408-aa3dd5efac3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73b6be63-d251-41c2-b4fb-052e629bcf77",
   "metadata": {},
   "source": [
    "### Function that takes grad tensor but do not take gradient calculation into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6328551e-c543-4e6d-ab26-b8e5581aec3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_tensor =  tensor([[ 3.,  6.,  9.],\n",
      "        [12., 15., 18.]])\n",
      "requires_grad for tensor1 =  True\n",
      "requires_grad for new_tensor =  False\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def mult_by_three(x):\n",
    "    y = x * 3\n",
    "    print('new_tensor = ', y)\n",
    "    print('requires_grad for tensor1 = ', x.requires_grad)\n",
    "    print('requires_grad for new_tensor = ', y.requires_grad)\n",
    "mult_by_three(tensor1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd899d0-e55c-4def-bbb2-f3c839b8be86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
